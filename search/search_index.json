{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"architecture/","title":"The architecture","text":"<p>Born with the following architectural idea</p> <p></p> <p>Basically the system consists of four main components that work together to enable intelligent GPU job scheduling:</p>"},{"location":"architecture/#1-job-submission-api","title":"1. Job submission API","text":"<p>RESTful API interface for submitting ML jobs to the scheduler.</p> <p>Should be built with FastAPI for high performance and easy integration with existing ML workflows ?</p>"},{"location":"architecture/#2-ml-predictor","title":"2. ML predictor","text":"<p>Resource estimation engine that analyzes incoming jobs and predicts:</p> <ul> <li> GPU memory usage</li> <li> Runtime duration</li> <li> Interference level between jobs</li> </ul>"},{"location":"architecture/#3-scheduler-core","title":"3. Scheduler core","text":"<p>Implement the sophisticated orchestration logic:</p> <ul> <li> Bin packing algorithm: Optimally assigns multiple jobs to available GPUs</li> <li> Interference-aware placement: Prevents resource contention by considering job compatibility</li> <li> MPS/MIG configuration: Leverages NVIDIA Multi-Process Service and Multi-Instance GPU technologies for isolation</li> </ul>"},{"location":"architecture/#4-job-executor","title":"4. Job executor","text":"<p>Manages the actual execution environment:</p> <ul> <li> Container orchestration and lifecycle management</li> <li> Isolated execution environments per job</li> <li> Resource allocation and enforcement</li> </ul>"},{"location":"architecture/#5-gpu-monitor","title":"5. GPU monitor","text":"<p>Real-time monitoring component that tracks:</p> <ul> <li> NVML (NVIDIA Management Library) metrics</li> <li> Live GPU utilization and memory usage</li> <li> Performance statistics for optimization feedback</li> </ul> <p>The GPU Monitor communicates bidirectionally with the Job Executor to provide runtime insights and enable dynamic adjustments.</p>"}]}